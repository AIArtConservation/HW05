# HW05

I chose the Salesforce "blip-image-captioning-base" model because I wanted to explore how to utilize images to generate text, specifically about what is inside the images. I have been working on a project for another class Iâ€™m taking this semester, where I use subway exit signs to generate AR content. So, I wanted to see if I could extract the text from subway signs and trains. However, I also wanted to conduct some generic testing on various kinds of objects beyond just trains and signs, to get a better understanding of what the model performs best on. To do this, I tested it with images of people, cats, and dogs. This helped me become more comfortable with actually using the model and running it. At first, I just used the example code provided on Hugging Face to understand how to use it and then customized it for my test cases.

Here are somethings I noticed: 
1. 
