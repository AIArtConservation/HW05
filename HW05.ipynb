{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW05: Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell if you need to install the PyTorch or Transformers libraries on a lab computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install diffusers transformers accelerate safetensors torchvision --upgrade\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports the libraries we need to run our inference pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth [Prediction](https://huggingface.co/depth-anything/Depth-Anything-V2-Base-hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_estimator = pipeline(\n",
    "  task=\"depth-estimation\",\n",
    "  model=\"depth-anything/Depth-Anything-V2-Base-hf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/flowers.jpg\")\n",
    "result = depth_estimator(image)\n",
    "display(result[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object [Detection](https://huggingface.co/facebook/detr-resnet-101)\n",
    "\n",
    "Some models don't work with the pipeline inference object, but the Transformers library still has some consistent-ish interfaces for running these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_MODEL = \"facebook/detr-resnet-101\"\n",
    "detr_processor = DetrImageProcessor.from_pretrained(OBJ_MODEL)\n",
    "detr_model = DetrForObjectDetection.from_pretrained(OBJ_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/people.jpg\")\n",
    "iw, ih = image.size\n",
    "\n",
    "detr_inputs = detr_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "detr_output = detr_model(**detr_inputs)\n",
    "detr_results = detr_processor.post_process_object_detection(detr_output, 0.99, [(ih, iw)])\n",
    "\n",
    "display(detr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn label ids into labels\n",
    "for label_id in detr_results[0][\"labels\"]:\n",
    "  print(detr_model.config.id2label[int(label_id)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to [Text](https://huggingface.co/Salesforce/blip-image-captioning-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a woman and her dog on the beach\n",
      "a woman sitting on the beach with her dog\n"
     ]
    }
   ],
   "source": [
    "#code from hugging face to test how it runs \n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "# >>> a photography of a woman and her dog\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - a train is stopped at a station in tokyo\n",
      "\n",
      "2 - a train is pulling passengers on the tracks\n",
      "\n",
      "3 - a train on the tracks\n",
      "\n",
      "4 - a train at a station with people standing around\n",
      "\n",
      "5 - a train on the tracks\n",
      "\n",
      "6 - a train is parked at the station\n",
      "\n",
      "7 - a train with a sign on the side of it\n",
      "\n",
      "8 - a train at the station in the city of cologne, germany - stock image\n",
      "\n",
      "9 - a train is coming down the tracks in a city\n",
      "\n",
      "10 - two trains on the tracks\n",
      "\n",
      "11 - a train traveling through the city of chicago\n",
      "\n",
      "12 - a train is parked at a station in the night\n",
      "\n",
      "13 - a train traveling down the tracks in a city\n",
      "\n",
      "14 - a train traveling down the tracks in a city\n",
      "\n",
      "15 - a train is stopped at a train station\n",
      "\n",
      "16 - a train traveling down the tracks in the snow\n",
      "\n",
      "17 - a train traveling down the tracks in the snow\n",
      "\n",
      "18 - a train traveling down the tracks in the snow\n",
      "\n",
      "19 - a train is coming down the tracks in the snow\n",
      "\n",
      "20 - a train is coming down the tracks in the snow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "\n",
    "#testing code\n",
    "\n",
    "#unprompted trains\n",
    "\n",
    "img_folder = \"/workspaces/HW05/imgs\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "train_imgs = [\n",
    "    \"A_train_02.jpg\", \"A_train_03.jpg\", \"A_train_04.webp\", \"A_train_05.webp\",\n",
    "    \"A_train_06.jpeg\", \"A_train_06.webp\", \"A_train_07.webp\", \"A_train_08.jpg\",\n",
    "    \"A_train_09.jpeg\", \"A_train_10.jpeg\", \"A_train_11.jpg\", \"A_train_12.webp\",\n",
    "    \"A_train_13.jpg\", \"A_train_14.webp\", \"A_train_15.jpg\", \"A_train_16.webp\",\n",
    "    \"A_train_17.webp\", \"A_train_18.webp\", \"A_train_19.jpg\", \"A_train_20.jpg\"\n",
    "]\n",
    "\n",
    "count = 1\n",
    "\n",
    "for photo in train_imgs:\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{count} - {caption}\\n\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - this new subway train type is the most in the world\n",
      "\n",
      "2 - this new subway train type is the only in the city\n",
      "\n",
      "3 - this new subway train type is red and white\n",
      "\n",
      "4 - this new subway train type is the first in the world to be built\n",
      "\n",
      "5 - this new subway train type is a high speed\n",
      "\n",
      "6 - this new subway train type is the most efficient in the world\n",
      "\n",
      "7 - this new subway train type is parked at the station\n",
      "\n",
      "8 - this new subway train type is the only used in the city of toronto, canada, and is now used for public transportation stock photo\n",
      "\n",
      "9 - this new subway train type is running through the city of kolkata\n",
      "\n",
      "10 - this new subway train type is a great way to get around the city\n",
      "\n",
      "11 - this new subway train type is seen in the lower manhattan area of new york, new york, new york, new york, new\n",
      "\n",
      "12 - this new subway train type is a little bit of a train\n",
      "\n",
      "13 - this new subway train type is the only available in the city of boston\n",
      "\n",
      "14 - this new subway train type is the most in the world\n",
      "\n",
      "15 - this new subway train type is a great example of the old style\n",
      "\n",
      "16 - this new subway train type is the most popular in the us\n",
      "\n",
      "17 - this new subway train type is the most popular in the us\n",
      "\n",
      "18 - this new subway train type is the most popular in the city\n",
      "\n",
      "19 - this new subway train type is coming to the station\n",
      "\n",
      "20 - this new subway train type is a real snow storm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prompted \n",
    "\n",
    "count = 1\n",
    "prompt = \"this new subway train type is \"  \n",
    "\n",
    "for photo in train_imgs:\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{count} - {caption}\\n\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "prompt = \"this new york city subway car type is  \"  \n",
    "\n",
    "for photo in train_imgs:\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"new prompt {count} - {caption}\\n\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animals unprompted and prompted\n",
    "\n",
    "animal_imgs = [\n",
    "    \"catdog.webp\", \"cats.webp\", \"dogs.jpg\"\n",
    "]\n",
    "\n",
    "count = 1\n",
    "\n",
    "for photo in animal_imgs:\n",
    "\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo} - {caption}\\n\")\n",
    "    count += 1\n",
    " \n",
    "count = 1\n",
    "#prompt = \"how many animals are in the picture \"\n",
    "prompt = \"Describe the animals in the image are \"\n",
    "\n",
    "# noticed that when i used colons the output with the prompt made no sense at all so I chnaged the prompt from \"the animal(s) in the image are:\"\" to above \n",
    "for photo1 in animal_imgs:\n",
    "    img_path = os.path.join(img_folder, photo1)  \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo1} - {caption}\\n\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#various races of people - unprompted and then prompted\n",
    "\n",
    "ppl_imgs = [\n",
    "    \"whitemen.webp\", \"eastasianmen.webp\", \"asian actresses.webp\", \"blackmen.jpg\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for photo in ppl_imgs:\n",
    "\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo} - {caption}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"the skin color of the people in the picture is \"\n",
    "\n",
    "# \n",
    "for photo1 in ppl_imgs:\n",
    "    img_path = os.path.join(img_folder, photo1)  \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo1} - {caption}\\n\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subway exit signs - unprompted & prompted extraction tried with just prompting first but didn't work well then I found out about tesseract an OCR tool\n",
    "import pytesseract\n",
    "\n",
    "sub_imgs = [\n",
    "    \"subway exit 1.jpg\",\"subway exit 2.webp\", \"subway_exit_signs.jpg\"\n",
    "]\n",
    "\n",
    "for photo in sub_imgs:\n",
    "\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo} - {caption}\\n\")\n",
    "\n",
    "prompt = \"the street name on the exit sign says \"\n",
    "#tried to extract the text but did not have a lot of luck\n",
    "for photo1 in sub_imgs:\n",
    "    img_path = os.path.join(img_folder, photo1)  \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    extracted_text = pytesseract.image_to_string(raw_image)\n",
    "    print(f\"Extracted text from {photo1}: {extracted_text}\\n\")\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo1} - {caption}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
