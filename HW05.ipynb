{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW05: Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell if you need to install the PyTorch or Transformers libraries on a lab computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install diffusers transformers accelerate safetensors torchvision --upgrade\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports the libraries we need to run our inference pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth [Prediction](https://huggingface.co/depth-anything/Depth-Anything-V2-Base-hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_estimator = pipeline(\n",
    "  task=\"depth-estimation\",\n",
    "  model=\"depth-anything/Depth-Anything-V2-Base-hf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/flowers.jpg\")\n",
    "result = depth_estimator(image)\n",
    "display(result[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object [Detection](https://huggingface.co/facebook/detr-resnet-101)\n",
    "\n",
    "Some models don't work with the pipeline inference object, but the Transformers library still has some consistent-ish interfaces for running these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_MODEL = \"facebook/detr-resnet-101\"\n",
    "detr_processor = DetrImageProcessor.from_pretrained(OBJ_MODEL)\n",
    "detr_model = DetrForObjectDetection.from_pretrained(OBJ_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/people.jpg\")\n",
    "iw, ih = image.size\n",
    "\n",
    "detr_inputs = detr_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "detr_output = detr_model(**detr_inputs)\n",
    "detr_results = detr_processor.post_process_object_detection(detr_output, 0.99, [(ih, iw)])\n",
    "\n",
    "display(detr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn label ids into labels\n",
    "for label_id in detr_results[0][\"labels\"]:\n",
    "  print(detr_model.config.id2label[int(label_id)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to [Text](https://huggingface.co/Salesforce/blip-image-captioning-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from hugging face to test how it runs \n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "# >>> a photography of a woman and her dog\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "\n",
    "#testing code\n",
    "\n",
    "#unprompted trains\n",
    "\n",
    "img_folder = \"/workspaces/HW05/imgs\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "train_imgs = [\n",
    "    \"A_train_02.jpg\", \"A_train_03.jpg\", \"A_train_04.webp\", \"A_train_05.webp\",\n",
    "    \"A_train_06.jpeg\", \"A_train_06.webp\", \"A_train_07.webp\", \"A_train_08.jpg\",\n",
    "    \"A_train_09.jpeg\", \"A_train_10.jpeg\", \"A_train_11.jpg\", \"A_train_12.webp\",\n",
    "    \"A_train_13.jpg\", \"A_train_14.webp\", \"A_train_15.jpg\", \"A_train_16.webp\",\n",
    "    \"A_train_17.webp\", \"A_train_18.webp\", \"A_train_19.jpg\", \"A_train_20.jpg\"\n",
    "]\n",
    "\n",
    "count = 1\n",
    "\n",
    "for photo in train_imgs:\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{count} - {caption}\\n\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompted \n",
    "\n",
    "count = 1\n",
    "prompt = \"this new subway train is \"  \n",
    "\n",
    "for photo in train_imgs:\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{count} - {caption}\\n\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "prompt = \"New York City Subway: this photo shows \"  \n",
    "\n",
    "for photo in train_imgs:\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"new prompt {count} - {caption}\\n\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catdog.webp - two pugs laying on a blanket with a cat\n",
      "\n",
      "cats.webp - a group of cats sitting in a circle\n",
      "\n",
      "dogs.jpg - three dogs are sitting next to each other dogs\n",
      "\n",
      "catdog.webp - how many animals are in the picture of a cat and a dog\n",
      "\n",
      "cats.webp - how many animals are in the picture\n",
      "\n",
      "dogs.jpg - how many animals are in the picture\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# animals unprompted and prompted\n",
    "\n",
    "animal_imgs = [\n",
    "    \"catdog.webp\", \"cats.webp\", \"dogs.jpg\"\n",
    "]\n",
    "\n",
    "count = 1\n",
    "\n",
    "for photo in animal_imgs:\n",
    "\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo} - {caption}\\n\")\n",
    "    count += 1\n",
    " \n",
    "count = 1\n",
    "#prompt = \"how many animals are in the picture \"\n",
    "prompt = \"Describe the animals in the image are \"\n",
    "\n",
    "# noticed that when i used colons the output with the prompt made no sense at all so I chnaged the prompt from \"the animal(s) in the image are:\"\" to above \n",
    "for photo1 in animal_imgs:\n",
    "    img_path = os.path.join(img_folder, photo1)  \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo1} - {caption}\\n\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitemen.webp - the cast of the vampire\n",
      "\n",
      "eastasianmen.webp - four men in suits and ties\n",
      "\n",
      "asian actresses.webp - the nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees nominees\n",
      "\n",
      "blackmen.jpg - the cast of the oscars\n",
      "\n",
      "whitemen.webp - the skin color of the people in the picture is the same\n",
      "\n",
      "eastasianmen.webp - the skin color of the people in the picture is the same, but the hair color is different\n",
      "\n",
      "asian actresses.webp - the skin color of the people in the picture is the most popular\n",
      "\n",
      "blackmen.jpg - the skin color of the people in the picture is a black man\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#various races of people - unprompted and then prompted\n",
    "\n",
    "ppl_imgs = [\n",
    "    \"whitemen.webp\", \"eastasianmen.webp\", \"asian actresses.webp\", \"blackmen.jpg\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for photo in ppl_imgs:\n",
    "\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo} - {caption}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"the skin color of the people in the picture is \"\n",
    "\n",
    "# \n",
    "for photo1 in ppl_imgs:\n",
    "    img_path = os.path.join(img_folder, photo1)  \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo1} - {caption}\\n\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/vscode/.local/lib/python3.10/site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/vscode/.local/lib/python3.10/site-packages (from pytesseract) (11.1.0)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subway exit 1.jpg - a sign on the floor in the subway station\n",
      "\n",
      "subway exit 2.webp - exit sign in subway station\n",
      "\n",
      "subway_exit_signs.jpg - a sign that says exit to the subway\n",
      "\n",
      "Extracted text from subway exit 1.jpg: \n",
      "\n",
      "subway exit 1.jpg - the street name on the exit sign says & amp & amp & amp & amp & amp & amp & amp & amp & amp & amp\n",
      "\n",
      "Extracted text from subway exit 2.webp: \n",
      "\n",
      "subway exit 2.webp - the street name on the exit sign says exit\n",
      "\n",
      "Extracted text from subway_exit_signs.jpg: Sage Sets\n",
      "\n",
      "Use last two\n",
      "\n",
      "stairways\n",
      "for) @eo\n",
      "\n",
      "\n",
      "\n",
      "subway_exit_signs.jpg - the street name on the exit sign says exit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subway exit signs - unprompted & prompted extraction\n",
    "import pytesseract\n",
    "\n",
    "sub_imgs = [\n",
    "    \"subway exit 1.jpg\",\"subway exit 2.webp\", \"subway_exit_signs.jpg\"\n",
    "]\n",
    "\n",
    "for photo in sub_imgs:\n",
    "\n",
    "    img_path = os.path.join(img_folder, photo) \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo} - {caption}\\n\")\n",
    "\n",
    "prompt = \"the street name on the exit sign says \"\n",
    "#tried to extract the text but did not have a lot of luck\n",
    "for photo1 in sub_imgs:\n",
    "    img_path = os.path.join(img_folder, photo1)  \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    extracted_text = pytesseract.image_to_string(raw_image)\n",
    "    print(f\"Extracted text from {photo1}: {extracted_text}\\n\")\n",
    "\n",
    "    inputs = processor(raw_image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"{photo1} - {caption}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
