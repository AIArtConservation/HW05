{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW05: Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell if you need to install the PyTorch or Transformers libraries on a lab computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install diffusers transformers accelerate safetensors torchvision --upgrade\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports the libraries we need to run our inference pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth [Prediction](https://huggingface.co/depth-anything/Depth-Anything-V2-Base-hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_estimator = pipeline(\n",
    "  task=\"depth-estimation\",\n",
    "  model=\"depth-anything/Depth-Anything-V2-Base-hf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/flowers.jpg\")\n",
    "result = depth_estimator(image)\n",
    "display(result[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object [Detection](https://huggingface.co/facebook/detr-resnet-101)\n",
    "\n",
    "Some models don't work with the pipeline inference object, but the Transformers library still has some consistent-ish interfaces for running these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_MODEL = \"facebook/detr-resnet-101\"\n",
    "detr_processor = DetrImageProcessor.from_pretrained(OBJ_MODEL)\n",
    "detr_model = DetrForObjectDetection.from_pretrained(OBJ_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/people.jpg\")\n",
    "iw, ih = image.size\n",
    "\n",
    "detr_inputs = detr_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "detr_output = detr_model(**detr_inputs)\n",
    "detr_results = detr_processor.post_process_object_detection(detr_output, 0.99, [(ih, iw)])\n",
    "\n",
    "display(detr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn label ids into labels\n",
    "for label_id in detr_results[0][\"labels\"]:\n",
    "  print(detr_model.config.id2label[int(label_id)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to [Text](https://huggingface.co/Salesforce/blip-image-captioning-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a woman and her dog on the beach\n",
      "a woman sitting on the beach with her dog\n"
     ]
    }
   ],
   "source": [
    "#code from hugging face to test how it runs \n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "# >>> a photography of a woman and her dog\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
